{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on workshop: Image classification with Keras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will learn how to build a convolutional neural network from scratch with Keras. You will build a simple network to classify one of the most well-known datasets in the world of image classification - the [MNIST database of handwritten digits](http://yann.lecun.com/exdb/mnist/). You can generalize the steps we follow in this tutorial to virtually any image dataset. Of course, with more complex images with more subtle differences between them, you will need to train much deeper and wider networks, for much longer periods of time. Luckily with transfer learning, you can use a pre-trained network trained on a *different* dataset, andfine-tune it to your particular dataset. We will see an ecample of this in the second part of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with the deep learning library Keras. Keras comes with a bunch of datasets that you can play around with, including MNIST. Let's start by installing and importing the libraries we need for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (18.1)\n",
      "Requirement already satisfied: tensorflow-gpu in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (1.11.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from tensorflow-gpu) (3.6.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from tensorflow-gpu) (1.16.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from tensorflow-gpu) (0.6.1)\n",
      "Requirement already satisfied: setuptools<=39.1.0 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from tensorflow-gpu) (39.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.5 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from tensorflow-gpu) (1.0.6)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.3 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from tensorflow-gpu) (1.0.5)\n",
      "Requirement already satisfied: gast>=0.2.0 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from tensorflow-gpu) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from tensorflow-gpu) (1.15.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from tensorflow-gpu) (1.11.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from tensorflow-gpu) (0.32.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from tensorflow-gpu) (0.7.1)\n",
      "Requirement already satisfied: tensorboard<1.12.0,>=1.11.0 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from tensorflow-gpu) (1.11.0)\n",
      "Requirement already satisfied: h5py in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from keras-applications>=1.0.5->tensorflow-gpu) (2.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow-gpu) (0.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow-gpu) (3.0.1)\n",
      "Requirement already satisfied: keras in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: h5py in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from keras) (2.8.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from keras) (1.0.5)\n",
      "Requirement already satisfied: pyyaml in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from keras) (3.13)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from keras) (1.0.6)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from keras) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from keras) (1.15.3)\n",
      "Requirement already satisfied: numpy in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (1.15.3)\n",
      "Requirement already satisfied: matplotlib in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (3.0.1)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from matplotlib) (1.15.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from matplotlib) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from matplotlib) (2.7.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib) (1.11.0)\n",
      "Requirement already satisfied: setuptools in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (39.1.0)\n",
      "Requirement already satisfied: scikit-learn in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (0.20.0)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: scipy in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from scipy) (1.15.3)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade pip\n",
    "!pip3 install tensorflow-gpu\n",
    "!pip3 install keras\n",
    "!pip3 install numpy\n",
    "!pip3 install matplotlib\n",
    "!pip3 install scikit-learn\n",
    "!pip3 install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vikram/miniconda3/envs/hcipy/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123) # For reproducibility\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth spending a bit of time here to look at the [documentation](https://keras.io/datasets/#mnist-database-of-handwritten-digits) of the Keras MNIST dataset. The load_data function gives you pre-shuffled MNIST data in train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (data) shape (60000, 28, 28)\n",
      "y_train (labels) shape (60000,)\n",
      "X_test (data) shape (10000, 28, 28)\n",
      "y_test (labels) shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load pre-shuffled MNIST data into train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Take a look at the shape of the data arrays\n",
    "print(\"X_train (data) shape\", X_train.shape)\n",
    "print(\"y_train (labels) shape\", y_train.shape)\n",
    "print(\"X_test (data) shape\", X_test.shape)\n",
    "print(\"y_test (labels) shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of the images in our dataset. We're going to use the matplotlib library and the function pyplot to plot our images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    index = np.random.randint(0, X_train.shape[0])\n",
    "    plt.imshow(X_train[index], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Label {}\".format(y_train[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Formatting the data so that we can feed it to the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When you are using Keras with TensorFlow as a backend, the input to a convolutional neural network is of the format (n_images, n_rows, n_cols, n_channels), where n_images is the numbner of images in your training set or batch, n_rows and n_cols are the number of rows and columns of pixels, and n_channels is the number of color channels (1 for black and white, 3 for colored images). As we have already seen that the format of the data is currently (n_images, n_rows, n_cols), we need to add an extra dimension for n_channels.  \n",
    "* It is always useful to normalize your input data, so that rather than ranging from 0 to 255, the pixel values in your image range from 0 to 1 instead. This makes it easier to train your network.\n",
    "* Finally, since our labels are categorical, and we need them to be numeric for our network to be able to calculate a loss, we one-hot-encode our labels. That is, convert each label into a vector as follows:  \n",
    "0 -> [1, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
    "1 -> [0, 1, 0, 0, 0, 0, 0, 0, 0]  \n",
    "2 -> [0, 0, 1, 0, 0, 0, 0, 0, 0]  \n",
    "etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input data\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Convert input data type to float\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize input data\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Building our neural network with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways of building a neural network with Keras. The [Sequential](https://keras.io/getting-started/sequential-model-guide/) method and the [Functional](https://keras.io/getting-started/functional-api-guide/) method. The Functional method is useful for building very complex networks with fancy connections, but we will look at the simpler Sequential method for now.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some parameters for the network\n",
    "batch_size = 128 # Change this to experiment - best to keep it a power of 2\n",
    "epochs = 12      # Change this to experiment - train as many epochs as you see the loss decreasing, but no more\n",
    "\n",
    "num_classes = 10 # This depends on your data. In MNIST, 10 digits from 0 to 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we declare a sequential model. Then we add some layers. This is the part where you experiment. Add different kinds of layers, with different amounts of filters. Experiment with dropout and normalization layers. Experiment with adding more layers. The [Keras documentation](https://keras.io/) is your friend, but for convenience I have added some useful info about commonly used layers right here:  \n",
    "* [Convolutional layers](https://keras.io/layers/convolutional/) - This is your standard convolution. You specify the number of filters and the kernel size, and if you need to you can change the stride and the padding.\n",
    "* [Max Pooling layers](https://keras.io/layers/pooling/#maxpooling2d) - These layers take a high resolution image and make it slightly lower resolution. A max pooling layer usually follows a convolutional layer.\n",
    "* [Batch Normalization](https://keras.io/layers/normalization/) - A very useful layer. It normalizes the input from the previous layer, to have close to 0 mean and unit standard deviation. This helps a lot in training networks faster, with the slight disadvantage of having more parameters to train.\n",
    "* [Dropout](https://keras.io/layers/core/#dropout) - This layer randomly turns off a percentage of the neurons entering it from the previous layer, at each epoch during training time. The net effect of this is to share weights more evenly across neurons, which results in less overfitting and makes the netowrk more likely to generalize to test data.\n",
    "* [Dense layers](https://keras.io/layers/core/#dense) - Your standard run-of-the-mill fully connected layer. After a bunch of convolutions and max-pooling to extract features, you will add a few dense layers to do the classification.  \n",
    "I have built a sample neural network below to show you how it's done, but feel free to experiment with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# First we declare a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers\n",
    "# 1 - Convolutional and pooling layers for learning features\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=X_train.shape[1:]))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "\n",
    "# Note that after each convolution and max pool the output height and width changes.\n",
    "\n",
    "# 2 - Fully connected layers for classifying the features\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Compile the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We configure the model for training with the model.compile command. We have to specify a loss, an optimizer, and the metrics we are measuring. As far as the loss goes, just remember that for a multi-label classification you would use binary cross-entropy whereas for a multi-class classification where each example belongs to a single class, you would use categorical cross entropy. The optimizer could be plain old gradient descent, but there are so many faster and more efficient optimizers out there now which you should definitely play around with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optimizer = optimizers.Adadelta(lr=1.0)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=model_optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the part where the weights of the model are adjusted via gradient descent (or a variation of gradient descent specified by the optimizer). With backprop, all the weights in the hidden layers are updated via the chain rule, and at each epoch your model's predictions become a bit better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 163s 3ms/step - loss: 0.2555 - acc: 0.9216 - val_loss: 0.0556 - val_acc: 0.9818\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 15s 253us/step - loss: 0.0862 - acc: 0.9742 - val_loss: 0.0367 - val_acc: 0.9873\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 15s 253us/step - loss: 0.0632 - acc: 0.9814 - val_loss: 0.0328 - val_acc: 0.9885\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 0.0524 - acc: 0.9839 - val_loss: 0.0303 - val_acc: 0.9892\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 0.0460 - acc: 0.9859 - val_loss: 0.0267 - val_acc: 0.9910\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 15s 257us/step - loss: 0.0391 - acc: 0.9881 - val_loss: 0.0283 - val_acc: 0.9901\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 16s 266us/step - loss: 0.0360 - acc: 0.9891 - val_loss: 0.0282 - val_acc: 0.9904\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 15s 250us/step - loss: 0.0345 - acc: 0.9892 - val_loss: 0.0266 - val_acc: 0.9912\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 15s 247us/step - loss: 0.0299 - acc: 0.9910 - val_loss: 0.0252 - val_acc: 0.9914\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 15s 250us/step - loss: 0.0304 - acc: 0.9904 - val_loss: 0.0266 - val_acc: 0.9913\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 15s 246us/step - loss: 0.0279 - acc: 0.9917 - val_loss: 0.0228 - val_acc: 0.9924\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 15s 247us/step - loss: 0.0256 - acc: 0.9922 - val_loss: 0.0244 - val_acc: 0.9921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6d489fd630>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.024429745590953098\n",
      "Test accuracy: 0.9921\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermezzo: Visualizing the outputs of the hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of people like to say that neural networks are a black box. That it is difficult to figure on what exactly a neural network is calculating. This makes them hesitant to use neural networks, and they go for more well-understood machine learning algorithms like support vector machines or decision trees. This is not a true statement, especially in the case of convolutional networks. One can easily see the outputs of each hidden layer for a particular input, because the hidden layer outputs are images themselves. You can refer to the excellent paper [Zeiler & Fergus(2013) - Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901) to see examples of hidden layer outputs. We will write a small function to do just that with our classifier.\n",
    "\n",
    "<img src=\"nb_images/CNN_feature_extract.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "def visualize_a_layer(image, n_layer, model_network):\n",
    "    '''Visualize the filters of layer n of a conv net.\n",
    "    image: the input image to the network\n",
    "    n_layer: the number of the layer to visualize\n",
    "    model_network: the neural network'''\n",
    "    \n",
    "    vis_model = Model([model_network.layers[0].input], [model_network.layers[n_layer].output])\n",
    "    vis_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=model_optimizer,\n",
    "              metrics=['accuracy'])\n",
    "    activations = vis_model.predict(image)[0]\n",
    "    \n",
    "    # The index 2 corresponds to the filter number so we traverse all filters\n",
    "    box = int(np.sqrt(activations.shape[2]))\n",
    "    for filt in range(activations.shape[2]):\n",
    "        plt.subplot(box,box,filt+1)\n",
    "        plt.imshow(activations[:,:,filt], cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 64 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_to_visualize = np.expand_dims(X_train[65], axis=0)\n",
    "visualize_a_layer(img_to_visualize, 2, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Transfer learning with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very nifty feature of convolutional neural netowrks is that the earlier layers learn very basic shapes, edges, and low-level features that are common to all objects. So a powerful neural network that has been trained on many images could be re-used for a totally different problem with a completely different dataset. We just fine-tune the last few layers of the network and we have a state-of-the-art classifier, while barely doing any work ourselves. The rest of this notebook will show you how to use the trained deep neural networks that are built into Keras - neural networks that have achieved record breaking accuracies at image classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the dataset\n",
    "We will use a set of [25,000 images of dogs and cats from Kaggle](https://www.kaggle.com/c/dogs-vs-cats/data). Our goal is to train a classifier to differentiate between dogs and cats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: kaggle in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (1.5.0)\r\n",
      "Requirement already satisfied, skipping upgrade: python-slugify in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from kaggle) (1.2.6)\r\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.23.0,>=1.15 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from kaggle) (1.22)\r\n",
      "Requirement already satisfied, skipping upgrade: requests in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from kaggle) (2.20.0)\r\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from kaggle) (2.7.3)\r\n",
      "Requirement already satisfied, skipping upgrade: certifi in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from kaggle) (2018.10.15)\r\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from kaggle) (1.11.0)\r\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from kaggle) (4.28.1)\r\n",
      "Requirement already satisfied, skipping upgrade: Unidecode>=0.04.16 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from python-slugify->kaggle) (1.0.22)\r\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from requests->kaggle) (3.0.4)\r\n",
      "Requirement already satisfied, skipping upgrade: idna<2.8,>=2.5 in /home/vikram/miniconda3/envs/workshops/lib/python3.6/site-packages (from requests->kaggle) (2.7)\r\n"
     ]
    }
   ],
   "source": [
    "# Install the Kaggle API\n",
    "!pip install kaggle --upgrade\n",
    "\n",
    "# Download the data\n",
    "import os\n",
    "if not os.path.exists(\"train.zip\"):\n",
    "    !kaggle competitions download -c dogs-vs-cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the files\n",
    "import zipfile\n",
    "zip_train = zipfile.ZipFile(\"train.zip\", 'r')\n",
    "zip_train.extractall()\n",
    "zip_train.close()\n",
    "\n",
    "zip_test = zipfile.ZipFile(\"test1.zip\", 'r')\n",
    "zip_test.extractall()\n",
    "zip_test.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image data augmentation\n",
    "If you inspect the data you will see that every image has a different width and height. We need a way to make them all the same shape to feed into the convnet. Also, we have plenty of images here, but it's always nicer to have more variety in our data. One way of doing this would be to randomly translate or rotate the images or mirror them. We can address both of these themes with data augmentation. Keras provides an image data augmentation class which we can use to get a larger dataset of same sized images from the training data.  \n",
    "To make things easier, let's first split the data into a training folder and a validation folder. Then, let's move all the dogs into a folder named dogs and cats into a folder named cats. An image is a dog if it has the word \"dog\" in its name and cat if it contains \"cat\" in its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import shutil\n",
    "from random import sample\n",
    "\n",
    "# Create a directory for validation data\n",
    "if not os.path.exists(\"validate\"):\n",
    "    os.mkdir(\"validate\")\n",
    "\n",
    "# Move 20% of training data to validation\n",
    "files = os.listdir(\"train/\")\n",
    "vfiles = sample(files, int(0.2 * len(files)))\n",
    "for f in vfiles:\n",
    "    shutil.move(\"train/\" + f, \"validate/\" + f)\n",
    "\n",
    "# Create separate folders for dogs and cats\n",
    "if not os.path.exists(\"train/dogs\"):\n",
    "    os.mkdir(\"train/dogs\")\n",
    "if not os.path.exists(\"train/cats\"):\n",
    "    os.mkdir(\"train/cats\")\n",
    "\n",
    "for image in glob.glob(\"train/cat.*\"):\n",
    "    shutil.move(image, \"train/cats/.\")\n",
    "for image in glob.glob(\"train/dog.*\"):\n",
    "    shutil.move(image, \"train/dogs/.\")\n",
    "\n",
    "if not os.path.exists(\"validate/dogs\"):\n",
    "    os.mkdir(\"validate/dogs\")\n",
    "if not os.path.exists(\"validate/cats\"):\n",
    "    os.mkdir(\"validate/cats\")\n",
    "\n",
    "for image in glob.glob(\"validate/cat.*\"):\n",
    "    shutil.move(image, \"validate/cats/.\")\n",
    "for image in glob.glob(\"validate/dog.*\"):\n",
    "    shutil.move(image, \"validate/dogs/.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up the Image Data Generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n",
      "Found 5000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "# The augmentation configuration we will use for training\n",
    "train_datagen = image.ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# The augmentation configuration we will use for testing:\n",
    "test_datagen = image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# This generator will generate augmented data from the training folder\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'train',  # this is the target directory\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=16,\n",
    "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# This is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'validate',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=16,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the pre-trained VGG16 model from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "\n",
    "VGGmodel = applications.VGG16(weights='imagenet', include_top=False, input_shape = (150,150,3))\n",
    "\n",
    "VGGmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add on a few extra layers at the end to fine-tune it for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a classifier model to put on top of the convolutional model\n",
    "# Here we need to use the functional method of building a neural network in Keras\n",
    "\n",
    "top_model = Flatten()(VGGmodel.output)\n",
    "top_model = Dense(256, activation='relu')(top_model)\n",
    "top_model = Dropout(0.5)(top_model)\n",
    "top_model = Dense(1, activation='sigmoid')(top_model)\n",
    "\n",
    "full_model = Model([VGGmodel.input], [top_model])\n",
    "# Set the first 16 layers to non-trainable (weights will not be updated)\n",
    "for layer in full_model.layers[:16]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.compile(loss='binary_crossentropy',\n",
    "              optimizer=model_optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "15/15 [==============================] - 5s 349ms/step - loss: 1.1779 - acc: 0.5708 - val_loss: 0.6782 - val_acc: 0.6667\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 4s 291ms/step - loss: 0.7000 - acc: 0.5542 - val_loss: 0.6140 - val_acc: 0.6667\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 4s 289ms/step - loss: 0.6294 - acc: 0.6833 - val_loss: 5.3801 - val_acc: 0.4479\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 4s 291ms/step - loss: 0.8251 - acc: 0.6167 - val_loss: 0.5147 - val_acc: 0.7917\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 4s 289ms/step - loss: 0.5521 - acc: 0.7167 - val_loss: 0.5605 - val_acc: 0.6562\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 4s 290ms/step - loss: 0.5434 - acc: 0.7333 - val_loss: 0.3161 - val_acc: 0.8854\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 4s 290ms/step - loss: 0.6070 - acc: 0.7500 - val_loss: 0.4227 - val_acc: 0.8021\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 4s 294ms/step - loss: 0.4999 - acc: 0.7583 - val_loss: 0.3320 - val_acc: 0.8542\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 4s 291ms/step - loss: 0.5087 - acc: 0.7750 - val_loss: 0.3183 - val_acc: 0.8750\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 4s 292ms/step - loss: 0.4576 - acc: 0.7792 - val_loss: 0.2771 - val_acc: 0.8750\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 4s 288ms/step - loss: 0.3549 - acc: 0.8333 - val_loss: 0.2340 - val_acc: 0.8750\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 4s 294ms/step - loss: 0.3299 - acc: 0.8458 - val_loss: 0.2424 - val_acc: 0.8542\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 4s 289ms/step - loss: 0.3302 - acc: 0.8625 - val_loss: 0.1836 - val_acc: 0.9167\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 4s 290ms/step - loss: 0.3582 - acc: 0.8750 - val_loss: 0.1613 - val_acc: 0.9167\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 4s 290ms/step - loss: 0.4748 - acc: 0.8542 - val_loss: 0.2020 - val_acc: 0.9167\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 4s 291ms/step - loss: 0.3985 - acc: 0.8500 - val_loss: 0.2836 - val_acc: 0.8542\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 4s 289ms/step - loss: 0.3247 - acc: 0.8542 - val_loss: 0.1697 - val_acc: 0.9271\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 4s 290ms/step - loss: 0.3380 - acc: 0.8542 - val_loss: 0.2052 - val_acc: 0.9167\n",
      "Epoch 19/50\n",
      "15/15 [==============================] - 4s 291ms/step - loss: 0.2946 - acc: 0.8750 - val_loss: 0.1696 - val_acc: 0.9583\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 4s 289ms/step - loss: 0.3498 - acc: 0.8500 - val_loss: 0.2246 - val_acc: 0.8958\n",
      "Epoch 21/50\n",
      "15/15 [==============================] - 4s 293ms/step - loss: 0.3077 - acc: 0.8667 - val_loss: 0.1667 - val_acc: 0.9375\n",
      "Epoch 22/50\n",
      "15/15 [==============================] - 4s 291ms/step - loss: 0.3583 - acc: 0.8542 - val_loss: 0.1783 - val_acc: 0.9479\n",
      "Epoch 23/50\n",
      "15/15 [==============================] - 4s 291ms/step - loss: 0.2825 - acc: 0.8542 - val_loss: 0.2103 - val_acc: 0.9167\n",
      "Epoch 24/50\n",
      "15/15 [==============================] - 4s 289ms/step - loss: 0.2881 - acc: 0.9000 - val_loss: 0.2023 - val_acc: 0.8958\n",
      "Epoch 25/50\n",
      "15/15 [==============================] - 4s 288ms/step - loss: 0.3410 - acc: 0.8542 - val_loss: 0.2976 - val_acc: 0.8542\n",
      "Epoch 26/50\n",
      "15/15 [==============================] - 4s 290ms/step - loss: 0.2615 - acc: 0.8958 - val_loss: 0.1679 - val_acc: 0.9271\n",
      "Epoch 27/50\n",
      "15/15 [==============================] - 4s 291ms/step - loss: 0.1937 - acc: 0.9125 - val_loss: 0.2445 - val_acc: 0.8958\n",
      "Epoch 28/50\n",
      "15/15 [==============================] - 4s 289ms/step - loss: 0.3289 - acc: 0.8833 - val_loss: 0.1568 - val_acc: 0.9167\n",
      "Epoch 29/50\n",
      "15/15 [==============================] - 4s 290ms/step - loss: 0.2527 - acc: 0.9083 - val_loss: 0.1909 - val_acc: 0.9167\n",
      "Epoch 30/50\n",
      "15/15 [==============================] - 4s 290ms/step - loss: 0.2571 - acc: 0.8875 - val_loss: 0.1674 - val_acc: 0.9271\n",
      "Epoch 31/50\n",
      "15/15 [==============================] - 4s 290ms/step - loss: 0.2707 - acc: 0.8833 - val_loss: 0.1598 - val_acc: 0.9583\n",
      "Epoch 32/50\n",
      "15/15 [==============================] - 4s 294ms/step - loss: 0.2742 - acc: 0.8792 - val_loss: 0.1991 - val_acc: 0.9375\n",
      "Epoch 33/50\n",
      "15/15 [==============================] - 4s 290ms/step - loss: 0.2980 - acc: 0.8917 - val_loss: 0.1365 - val_acc: 0.9375\n",
      "Epoch 34/50\n",
      "15/15 [==============================] - 4s 289ms/step - loss: 0.2527 - acc: 0.8958 - val_loss: 0.1199 - val_acc: 0.9479\n",
      "Epoch 35/50\n",
      "15/15 [==============================] - 4s 290ms/step - loss: 0.2633 - acc: 0.8917 - val_loss: 0.1220 - val_acc: 0.9688\n",
      "Epoch 36/50\n",
      "15/15 [==============================] - 4s 290ms/step - loss: 0.3069 - acc: 0.8667 - val_loss: 0.1314 - val_acc: 0.9583\n",
      "Epoch 37/50\n",
      "15/15 [==============================] - 4s 290ms/step - loss: 0.2726 - acc: 0.9000 - val_loss: 0.1217 - val_acc: 0.9583\n",
      "Epoch 38/50\n",
      "15/15 [==============================] - 4s 289ms/step - loss: 0.1845 - acc: 0.9375 - val_loss: 0.1132 - val_acc: 0.9479\n",
      "Epoch 39/50\n",
      "15/15 [==============================] - 4s 291ms/step - loss: 0.3792 - acc: 0.8875 - val_loss: 0.1329 - val_acc: 0.9375\n",
      "Epoch 40/50\n",
      "15/15 [==============================] - 4s 290ms/step - loss: 0.2336 - acc: 0.8792 - val_loss: 0.1134 - val_acc: 0.9479\n",
      "Epoch 41/50\n",
      "15/15 [==============================] - 4s 289ms/step - loss: 0.2750 - acc: 0.9125 - val_loss: 0.1595 - val_acc: 0.9375\n",
      "Epoch 42/50\n",
      "15/15 [==============================] - 4s 290ms/step - loss: 0.3349 - acc: 0.8375 - val_loss: 0.1313 - val_acc: 0.9583\n",
      "Epoch 43/50\n",
      "15/15 [==============================] - 4s 291ms/step - loss: 0.2497 - acc: 0.8792 - val_loss: 0.1105 - val_acc: 0.9688\n",
      "Epoch 44/50\n",
      "15/15 [==============================] - 4s 289ms/step - loss: 0.2674 - acc: 0.8750 - val_loss: 0.1108 - val_acc: 0.9583\n",
      "Epoch 45/50\n",
      "15/15 [==============================] - 4s 289ms/step - loss: 0.2329 - acc: 0.9333 - val_loss: 0.0999 - val_acc: 0.9688\n",
      "Epoch 46/50\n",
      "15/15 [==============================] - 4s 289ms/step - loss: 0.3136 - acc: 0.8625 - val_loss: 0.1234 - val_acc: 0.9583\n",
      "Epoch 47/50\n",
      "15/15 [==============================] - 4s 290ms/step - loss: 0.2175 - acc: 0.9042 - val_loss: 0.0979 - val_acc: 0.9583\n",
      "Epoch 48/50\n",
      "15/15 [==============================] - 4s 289ms/step - loss: 0.2903 - acc: 0.8750 - val_loss: 0.1166 - val_acc: 0.9479\n",
      "Epoch 49/50\n",
      "15/15 [==============================] - 4s 290ms/step - loss: 0.2210 - acc: 0.9000 - val_loss: 0.1214 - val_acc: 0.9479\n",
      "Epoch 50/50\n",
      "15/15 [==============================] - 4s 292ms/step - loss: 0.2746 - acc: 0.8917 - val_loss: 0.1343 - val_acc: 0.9479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6c9d4e8e10>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000 // batch_size,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hcipy]",
   "language": "python",
   "name": "conda-env-hcipy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
